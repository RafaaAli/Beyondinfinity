{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtxuLA2+mZbGYx7Q5nn/7M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Moe-phantom/Beyondinfinity/blob/main/TESS_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "importing the necassary **libraries**"
      ],
      "metadata": {
        "id": "5VKc_y-BuxC0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EWJeQPyN3kKz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/TOI_2025.09.28_05.51.22.csv',\n",
        "                 comment='#',            # ignore lines that start with #\n",
        "                 on_bad_lines='skip')\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)"
      ],
      "metadata": {
        "id": "ytqp2WEH32GZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section of the preprocessing script addresses severe class imbalance, which is common in astronomical catalogs where Planet Candidates (PC) often vastly outnumber Confirmed Planets or False Positives.\n",
        "\n",
        "Ambiguous Class Removal: Rows classified as APC (Ambiguous Planet Candidate) are initially removed to simplify the classification task and improve label quality.\n",
        "\n",
        "Downsampling: The script identifies the PC class (the majority class) and randomly samples a fixed number of rows (2,400) from it. All other minority classes (Confirmed Planets, False Positives) are retained in full.\n",
        "\n",
        "Recombination: The downsampled PC set is then merged back with the complete set of other classes."
      ],
      "metadata": {
        "id": "zbbzkxFmvhKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Remove ambiguous classes first\n",
        "df = df[~df['tfopwg_disp'].isin(['APC'])]\n",
        "# Separate the classes\n",
        "df_pc = df[df['tfopwg_disp'] == 'PC']          # Planet Candidates (to downsample)\n",
        "df_other = df[df['tfopwg_disp'] != 'PC']       # All other classes (keep all)\n",
        "\n",
        "# Downsample PC to 2,700\n",
        "df_pc_downsampled = df_pc.sample(n=2400, random_state=42)\n",
        "\n",
        "# Combine\n",
        "df = pd.concat([df_pc_downsampled, df_other], ignore_index=True)\n",
        "\n",
        "# Display new class distribution\n",
        "print(\"Class distribution after downsampling PC to 2,700:\")\n",
        "print(df['tfopwg_disp'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puIux7Nt3_cb",
        "outputId": "17131699-d574-4639-8822-cfb5911a861e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution after downsampling PC to 2,700:\n",
            "tfopwg_disp\n",
            "PC    2400\n",
            "FP    1196\n",
            "CP     683\n",
            "KP     583\n",
            "FA      98\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Check class distribution\n",
        "print(\"\\n1. Class Distribution:\")\n",
        "print(df['tfopwg_disp'].value_counts())\n",
        "print(\"\\nPercentages:\")\n",
        "print(df['tfopwg_disp'].value_counts(normalize=True) * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67C1wK1Q5kwb",
        "outputId": "e6476aaf-ab5f-46b3-c39f-93be8047042f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. Class Distribution:\n",
            "tfopwg_disp\n",
            "PC    2400\n",
            "FP    1196\n",
            "CP     683\n",
            "KP     583\n",
            "FA      98\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Percentages:\n",
            "tfopwg_disp\n",
            "PC    48.387097\n",
            "FP    24.112903\n",
            "CP    13.770161\n",
            "KP    11.754032\n",
            "FA     1.975806\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "his code block implements Feature Engineering, which is the process of creating new variables from existing data to improve the performance and interpretability of a machine learning model.\n",
        "\n",
        "The goal here is to introduce physics-informed features relevant to exoplanet detection"
      ],
      "metadata": {
        "id": "XERVFyhPwJpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"⚙️ ADVANCED FEATURE ENGINEERING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def engineer_features(df):\n",
        "    \"\"\"\n",
        "    Create physics-informed features that improve classification\n",
        "    Based on exoplanet detection theory\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    print(\"Creating engineered features...\")\n",
        "\n",
        "    # 1. Transit Signal-to-Noise (key discriminator!)\n",
        "    if 'pl_trandep' in df.columns and 'st_tmag' in df.columns:\n",
        "        df['transit_snr'] = df['pl_trandep'] * (10 ** (-df['st_tmag'] / 5))\n",
        "        print(\" ✅ transit_snr (Photon-noise-limited SNR)\")\n",
        "\n",
        "    # 4. Planet temperature ratio\n",
        "    if 'pl_eqt' in df.columns and 'st_teff' in df.columns:\n",
        "        df['temp_ratio'] = df['pl_eqt'] / df['st_teff']\n",
        "        print(\"   ✅ temp_ratio (Planet/Star temperature)\")\n",
        "\n",
        "\n",
        "    # 7. Log-transformed features (planets follow power laws!)\n",
        "    log_cols = ['pl_orbper', 'pl_rade', 'pl_insol']\n",
        "    for col in log_cols:\n",
        "        if col in df.columns:\n",
        "            df[f'{col}_log'] = np.log10(df[col] + 1e-6)\n",
        "            print(f\"   ✅ {col}_log\")\n",
        "\n",
        "    # 8. Interaction features (non-linear relationships!)\n",
        "    if 'st_mass' in df.columns and 'st_rad' in df.columns:\n",
        "        df['stellar_density'] = df['st_mass'] / (df['st_rad'] ** 3 + 1e-6)\n",
        "        print(\"   ✅ stellar_density\")\n",
        "\n",
        "    return df\n",
        "\n",
        "df_engineered = engineer_features(df)\n",
        "\n",
        "print(f\"\\n✅ Original features: {len(df.columns)}\")\n",
        "print(f\"✅ Total features now: {len(df_engineered.columns)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F--Qnjcr7G4z",
        "outputId": "484f0170-d33c-4dea-a391-17a4fd2d42bb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "⚙️ ADVANCED FEATURE ENGINEERING\n",
            "================================================================================\n",
            "Creating engineered features...\n",
            " ✅ transit_snr (Photon-noise-limited SNR)\n",
            "   ✅ temp_ratio (Planet/Star temperature)\n",
            "   ✅ pl_orbper_log\n",
            "   ✅ pl_rade_log\n",
            "   ✅ pl_insol_log\n",
            "\n",
            "✅ Original features: 27\n",
            "✅ Total features now: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Target Label Coarsening (Hierarchical Classification)\n",
        "This crucial step simplifies the multi-class prediction problem to maximize initial model accuracy.\n",
        "\n",
        "Instead of training the model to distinguish between all fine-grained classifications (like FP, FA, CP, etc.), this function implements a hierarchical strategy by grouping them into three robust, coarse categories:\n",
        "\n"
      ],
      "metadata": {
        "id": "eTKwQWEHwNQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"🎯 HIERARCHICAL CLASSIFICATION STRATEGY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def create_coarse_labels(df, target_col='tfopwg_disp'):\n",
        "    \"\"\"\n",
        "    Map 6 fine classes to 3 coarse classes\n",
        "    This is the KEY to high accuracy!\n",
        "    \"\"\"\n",
        "    mapping = {\n",
        "        'FP': 'NOT_PLANET',      # False Positive\n",
        "        'FA': 'NOT_PLANET',      # False Alarm\n",
        "        'PC': 'CANDIDATE',       # Planet Candidate\n",
        "        'CP': 'PLANET',          # Confirmed Planet\n",
        "        'KP': 'PLANET'           # Known Planet\n",
        "    }\n",
        "\n",
        "    df['coarse_class'] = df[target_col].map(mapping)\n",
        "\n",
        "    print(\"✅ Coarse Classification Mapping:\")\n",
        "    print(\"   NOT_PLANET ← [FP, FA]\")\n",
        "    print(\"   CANDIDATE  ← [PC]\")\n",
        "    print(\"   PLANET     ← [CP, KP]\")\n",
        "\n",
        "    return df\n",
        "\n",
        "df_engineered = create_coarse_labels(df_engineered)\n",
        "\n",
        "print(\"\\n📊 Coarse Class Distribution:\")\n",
        "print(df_engineered['coarse_class'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJV4Fo7j6d11",
        "outputId": "8bf36248-03ad-41a5-e4ac-e4b7e3998847"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 HIERARCHICAL CLASSIFICATION STRATEGY\n",
            "================================================================================\n",
            "✅ Coarse Classification Mapping:\n",
            "   NOT_PLANET ← [FP, FA]\n",
            "   CANDIDATE  ← [PC]\n",
            "   PLANET     ← [CP, KP]\n",
            "\n",
            "📊 Coarse Class Distribution:\n",
            "coarse_class\n",
            "CANDIDATE     2400\n",
            "NOT_PLANET    1294\n",
            "PLANET        1266\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This phase finalizes the dataset, ensuring only high-quality, relevant features are passed to the machine learning model.\n",
        "\n",
        "Feature Selection: Identifier columns (toi, tid, ra, dec) and the target labels (tfopwg_disp, coarse_class) are explicitly excluded. The remaining columns, including the newly engineered features, are selected as the final input feature set.\n",
        "\n",
        "Missing Value Handling: Missing data points (NaN) within the selected feature columns are addressed using Median Imputation. This replaces missing values with the median of their respective columns, preventing data loss while mitigating the influence of potential outliers.\n",
        "\n",
        "Infinity Removal: Any extreme, non-numeric infinite values (±∞) that may have resulted from mathematical operations (like division by zero during feature engineering) are removed.\n",
        "\n",
        "Result: A fully cleaned dataset with a complete, consistent set of numerical features, ready for scaling and training."
      ],
      "metadata": {
        "id": "nf5A0OeVxK6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 4. DATA PREPROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "# Select features (exclude identifiers and targets)\n",
        "exclude_cols = ['toi', 'tid', 'ra', 'dec', 'tfopwg_disp', 'coarse_class']\n",
        "feature_cols = [col for col in df_engineered.columns if col not in exclude_cols]\n",
        "\n",
        "print(f\"\\n📋 Using {len(feature_cols)} features for classification\")\n",
        "\n",
        "# Handle missing values\n",
        "df_clean = df_engineered.copy()\n",
        "for col in feature_cols:\n",
        "    if df_clean[col].isnull().sum() > 0:\n",
        "        df_clean[col].fillna(df_clean[col].median(), inplace=True)\n",
        "\n",
        "# Remove infinite values\n",
        "df_clean = df_clean.replace([np.inf, -np.inf], np.nan)\n",
        "df_clean = df_clean.dropna(subset=feature_cols)\n",
        "\n",
        "print(f\"✅ Clean dataset: {len(df_clean)} samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGRaBMoH7eDE",
        "outputId": "68976e9c-18ac-488a-9422-f84653f8d91a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📋 Using 27 features for classification\n",
            "✅ Clean dataset: 4960 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deciedig the neumaric feaatures"
      ],
      "metadata": {
        "id": "hYSTlwyMxOR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features = df_clean[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "print(f\"\\n✅ Keeping {len(numeric_features)} numeric features:\")\n",
        "print(numeric_features)\n",
        "\n",
        "# Update your feature columns\n",
        "feature_cols = numeric_features\n",
        "\n",
        "# Now this will work\n",
        "X = df_clean[feature_cols].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20woq0-o7JDB",
        "outputId": "91648d0d-bce3-4385-d39f-beb3b3eeefb8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Keeping 23 numeric features:\n",
            "['rowid', 'toipfx', 'ctoi_alias', 'pl_pnum', 'st_pmra', 'st_pmdec', 'pl_tranmid', 'pl_orbper', 'pl_trandurh', 'pl_trandep', 'pl_rade', 'pl_insol', 'pl_eqt', 'st_tmag', 'st_dist', 'st_teff', 'st_logg', 'st_rad', 'transit_snr', 'temp_ratio', 'pl_orbper_log', 'pl_rade_log', 'pl_insol_log']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "preparing the model for training"
      ],
      "metadata": {
        "id": "PQ6O9pZgxWyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"🎯 STAGE 1: COARSE CLASSIFICATION (3 classes)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "X = df_clean[feature_cols].values\n",
        "y_coarse = df_clean['coarse_class'].values\n",
        "\n",
        "# Encode labels\n",
        "le_coarse = LabelEncoder()\n",
        "y_coarse_encoded = le_coarse.fit_transform(y_coarse)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_coarse_encoded, test_size=0.2, random_state=RANDOM_STATE, stratify=y_coarse_encoded\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Training set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n",
        "\n",
        "# Train ensemble models\n",
        "print(\"\\n🤖 Training Stage 1 Models...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGObeftK7Sh5",
        "outputId": "0ee02469-b7c2-4d2b-fd93-821e28545d5c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "🎯 STAGE 1: COARSE CLASSIFICATION (3 classes)\n",
            "================================================================================\n",
            "Training set: (3968, 23)\n",
            "Test set: (992, 23)\n",
            "\n",
            "🤖 Training Stage 1 Models...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**XGBoost** training"
      ],
      "metadata": {
        "id": "ETLNZeVtxaOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Model 1: XGBoost (usually best for tabular data)\n",
        "print(\"\\n1️⃣ XGBoost...\")\n",
        "xgb1 = XGBClassifier(\n",
        "    n_estimators=500,\n",
        "    max_depth=8,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=RANDOM_STATE,\n",
        "    eval_metric='mlogloss',\n",
        "    scale_pos_weight=1\n",
        ")\n",
        "xgb1.fit(X_train_scaled, y_train)\n",
        "acc_xgb1 = accuracy_score(y_test, xgb1.predict(X_test_scaled))\n",
        "print(f\"   Accuracy: {acc_xgb1:.4f} ({acc_xgb1*100:.2f}%)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpoWk5b27phU",
        "outputId": "da763a10-f7e1-4837-b3bd-c51482d8625b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1️⃣ XGBoost...\n",
            "   Accuracy: 0.7591 (75.91%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LightGBM training"
      ],
      "metadata": {
        "id": "d0jk3unZxfTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 2: LightGBM (faster, often better)\n",
        "print(\"\\n2️⃣ LightGBM...\")\n",
        "lgb1 = LGBMClassifier(\n",
        "    n_estimators=500,\n",
        "    boosting_type='dart',\n",
        "    max_depth=8,\n",
        "    learning_rate=0.05,\n",
        "    random_state=RANDOM_STATE,\n",
        "    num_class=3,\n",
        "    verbose=-1\n",
        ")\n",
        "lgb1.fit(X_train_scaled, y_train)\n",
        "acc_lgb1 = accuracy_score(y_test, lgb1.predict(X_test_scaled))\n",
        "print(f\"   Accuracy: {acc_lgb1:.4f} ({acc_lgb1*100:.2f}%)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qs0Sa9D18FjW",
        "outputId": "b1941706-7cda-476c-9f08-a4abebf14019"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2️⃣ LightGBM...\n",
            "   Accuracy: 0.7530 (75.30%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "model evaluation and saving"
      ],
      "metadata": {
        "id": "BsMLPiv5xkGd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6a8675a",
        "outputId": "ca491a3d-81a2-4a8d-d378-88a767f6621b"
      },
      "source": [
        "# Evaluate models and save them\n",
        "print(\"\\n📊 Evaluating Stage 1 Models...\")\n",
        "\n",
        "# Evaluate XGBoost\n",
        "y_pred_xgb1 = xgb1.predict(X_test_scaled)\n",
        "print(\"\\nXGBoost Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_xgb1, target_names=le_coarse.classes_))\n",
        "print(\"XGBoost Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_xgb1))\n",
        "\n",
        "\n",
        "# Evaluate LightGBM\n",
        "y_pred_lgb1 = lgb1.predict(X_test_scaled)\n",
        "print(\"\\nLightGBM Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_lgb1, target_names=le_coarse.classes_))\n",
        "print(\"LightGBM Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_lgb1))\n",
        "\n",
        "# Save models\n",
        "print(\"\\n💾 Saving Stage 1 Models...\")\n",
        "joblib.dump(xgb1, 'xgb_coarse_model.pkl')\n",
        "joblib.dump(lgb1, 'lgb_coarse_model.pkl')\n",
        "print(\"✅ Models saved: xgb_coarse_model.pkl, lgb_coarse_model.pkl\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluating Stage 1 Models...\n",
            "\n",
            "XGBoost Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   CANDIDATE       0.73      0.87      0.79       480\n",
            "  NOT_PLANET       0.80      0.65      0.72       259\n",
            "      PLANET       0.79      0.66      0.72       253\n",
            "\n",
            "    accuracy                           0.76       992\n",
            "   macro avg       0.78      0.73      0.74       992\n",
            "weighted avg       0.77      0.76      0.76       992\n",
            "\n",
            "XGBoost Confusion Matrix:\n",
            "[[418  30  32]\n",
            " [ 79 168  12]\n",
            " [ 75  11 167]]\n",
            "\n",
            "LightGBM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   CANDIDATE       0.73      0.86      0.79       480\n",
            "  NOT_PLANET       0.80      0.64      0.71       259\n",
            "      PLANET       0.76      0.67      0.71       253\n",
            "\n",
            "    accuracy                           0.75       992\n",
            "   macro avg       0.76      0.72      0.74       992\n",
            "weighted avg       0.76      0.75      0.75       992\n",
            "\n",
            "LightGBM Confusion Matrix:\n",
            "[[411  30  39]\n",
            " [ 78 166  15]\n",
            " [ 72  11 170]]\n",
            "\n",
            "💾 Saving Stage 1 Models...\n",
            "✅ Models saved: xgb_coarse_model.pkl, lgb_coarse_model.pkl\n"
          ]
        }
      ]
    }
  ]
}